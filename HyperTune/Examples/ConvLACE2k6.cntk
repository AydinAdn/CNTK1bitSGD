#exp=ConvLACE  #provide it through Philly submit script

# this one uses simple LACE without ReLU.

command="TrainNewModel"

#config info put here so that we can determine the log and model file based on them
              
        _baseFeatDim = 40
        _leftFeatContext = 30
        _rightFeatContext = 30
        
        _labelDim = 9000;

        _numConvJumpBlocks = 4
        _startChannel = 128
        _numJumpLayersPerBlock = (2:2:2:2:2:2)  # 0-based, must have _numConvJumpBlocks+1 items
        _kW = (3:3:3:3:3:3) 
        _kH = (3:3:3:3:3:3) 
        _useMask = false  #true if each frame is weighted differently.
        
        _numLACELayers = 0;
        
        _DNNHiddenDim = 2048;  
        _numDNNLayersAfterConv = 0;

        _LSTMCellDim = 1024;        
        _LSTMProjDim = 512;
        _numLSTMLayers = 0
                        
        dropValue = false;
        dropLink = false;

# if _numLSTMLayers == 0 use following
        frameMode=true             
        minibatchSize=256:512

# else if _numLSTMLayers > 0 use following 
#        frameMode=false
#        Truncated=true
#        nbrUttsInEachRecurrentIter=8    # when use distributed training, increase this value instead of minibatchSize         
#        minibatchSize=16   # nbrUttsInEachRecurrentIter * minibatchSize is actual size

        batchNormalizationTimeConstant=12800*40:64000   #8640*40:86400:80:inf    #1#INF (infinity)means running values are "frozen"
        batchNormalizationBlendTimeConstant=0   #0*40:8640*40:86400*40:inf    #1#INF (infinity)means only running mean / var will be used
        learningRatesPerSample=0.004:0.008
        momentumPerMB=0:0.9
        maxEpochs=200
        
#        maxTempMemSizeInSamplesForCNN = 1

#end config info        

ModelName=$exp$_f$_baseFeatDim$l$_leftFeatContext$r$_rightFeatContext$_sc$_startChannel$_pb$_numConvJumpBlocks$_lace$_numLACELayers$_h$_DNNHiddenDim$X$_numDNNLayersAfterConv$_lstm$_LSTMCellDim$X$_LSTMProjDim$X$_numLSTMLayers$


RootDir=\\storage.gcr.philly.selfhost.corp.microsoft.com\pnrsy_scratch\dongyu

# For local debug
#DataDir=D:\temp\PhillyScripts\LocalData
#_numMBsToShowResult=1000

# For actual run
DataDir=\\storage.gcr.philly.selfhost.corp.microsoft.com\pnrsy\jdroppo\fisher_swbd300\segv2-FBANK_Z
_numMBsToShowResult=100

# Shared for local and Philly

ConfigDir=$RootDir$\config
ExpRoot=$RootDir$\exp
ModelDir=$ExpRoot$\$ModelName$
LogDir=$RootDir$\log

scp=$DataDir$/features.train.rscp
lab=$DataDir$/labels.realign.smlf
ivectors=$DataDir$/ivector.train.rscp
testscp=$DataDir$/features.dev.rscp
testlab=$DataDir$/labels.dev.smlf
testivectors=$DataDir$/ivector.dev.rscp
map=$DataDir$/labels.statelist

ivmean=$DataDir$/ivector.xMean.ascii
ivstddev=$DataDir$/ivector.xStdDev.ascii

meanFileName=$DataDir$/featNorm.xMean.ascii
stdFileName=$DataDir$/featNorm.xStdDev.ascii
labelPriorFileName=$DataDir$/Label.Prior.ascii

stderr=$LogDir$/log$exp$

shareNodeValueMatrices=true
parallelTrain=true

#######################################
#  TRAINING CONFIG (NDL, Fixed LR) #
#######################################
TrainNewModel = [

minimizeReaderMemoryFootprint = false 

    action="train"

    modelPath="$ModelDir$/$exp$.dnn"

    deviceId="Auto"

    traceLevel=1
    
    SGD=[
        keepCheckPointFiles = true
     
        epochSize=8640000    # 24hr data  #16384000
#       momentumAsTimeConstant=2500
        numMBsToShowResult=$_numMBsToShowResult$

        gradUpdateType="fsadagrad"
#        gradUpdateType="adagrad"
#        normWithAveMultiplier=true
        
#        UseAllDataForPreComputedNode=true
#        gradientClippingWithTruncation=true
#        clippingThresholdPerSample=0.1

        autoAdjust=[
            autoAdjustLR=AdjustAfterEpoch
            learnRateDecreaseFactor=0.7
            learnRateAdjustInterval=3
#           EmergencyReloadThreshold=0.3
        ]
        
        ParallelTrain=[
            parallelizationMethod="DataParallelSGD"
#           distributedMBReading=true
            parallelizationStartEpoch=1
            
            DataParallelSGD=[
                gradientBits=1
            ]
        ]
    ]


    # Parameter values for the reader
    reader = [
        readerType="HTKMLFReader"
        readMethod="blockRandomize"
        miniBatchMode="full"
		randomize=17280000   #48 hrs
        verbosity=0
        
        net.features = [
			dim=$_baseFeatDim$
			contextWindow=$_leftFeatContext$:$_rightFeatContext$	
            scpFile="$scp$"
            type="real"
        ]
        
        net.labels=[
            mlfFile="$lab$"
            labelMappingFile="$map$"
            labelDim=$_labelDim$
            labelType="category"
        ]
    ]
    
    cvreader=[
        readerType="HTKMLFReader"
        readMethod="blockRandomize"
        miniBatchMode="partial"
        randomize="auto"
        verbosity=0

        net.features=[
			dim=$_baseFeatDim$
			contextWindow=$_leftFeatContext$:$_rightFeatContext$	
            scpFile="$testscp$"
            type="real"
        ]
        
        net.labels=[
            mlfFile="$testlab$"
            labelMappingFile="$map$"
            labelDim=$_labelDim$
            labelType="category"
        ]
    ]


    BrainScriptNetworkBuilder = [ 
        includePath = "$ConfigDir$"
        include "$ConfigDir$/ConvLACE.bs";

        baseFeatDim = $_baseFeatDim$ 
        leftFeatContext = $_leftFeatContext$
        rightFeatContext = $_rightFeatContext$
        
        labelDim = $_labelDim$;

        numConvJumpBlocks = $_numConvJumpBlocks$
        startChannel = $_startChannel$
        kW = $_kW$
        kH = $_kH$
        useMask = "$_useMask$"
        numJumpLayersPerBlock = $_numJumpLayersPerBlock$

        numLACELayers = $_numLACELayers$
        DNNHiddenDim = $_DNNHiddenDim$;  
        numDNNLayersAfterConv = $_numDNNLayersAfterConv$;

        #dims used in LSTM models only 
        LSTMCellDim = $_LSTMCellDim$;        
        LSTMProjDim = $_LSTMProjDim$;
        numLSTMLayers = $_numLSTMLayers$;
        
        initWScale = 2.0;  # change to a larger value may be useful for some tasks
        initBias = 0.0;
        initBNScale = 1.0;
                
        net = CreateNetwork_Conv_LACE(baseFeatDim, leftFeatContext, rightFeatContext, labelDim, DNNHiddenDim, LSTMCellDim, LSTMProjDim, startChannel, kW, kH, numJumpLayersPerBlock, numConvJumpBlocks, numLACELayers, numDNNLayersAfterConv, numLSTMLayers, "$labelPriorFileName$", useMask, initWScale, initBias, initBNScale, $dropValue$, $dropLink$);
               
        #currently BrainScript need all root nodes exposed here
        ce = net.o.ce;
        err = net.o.err;
        scaledLogLikelihood = net.o.scaledLogLikelihood;
    ]
    
]

